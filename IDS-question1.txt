Question 1 Using IRIS data:


library(datasets)
iris_data <-iris
summary(iris_data)




>>>output 

Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
 Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
 Median :5.800   Median :3.000   Median :4.350   Median :1.300  
 Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
 Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
       Species  
 setosa    :50  
 versicolor:50  
 virginica :50 




plot(iris_data)
iris_data$Species <- factor(iris_data$Species)

#splitting dataset into training and testing
#install.packages('caTools')
library(caTools)
set.seed(123)
split <- sample.split(iris_data$Species,SplitRatio=0.8)
training_set <- subset(iris_data,split==TRUE)
test_set <- subset(iris_data,split==FALSE)


#feature scaling
training_set[-5]<- scale(training_set[,-5])
test_set[-5]<-scale(test_set[-5])


#fitting knn to the training set and predicting the test set results
library(class)
y_pred <- knn(train = training_set[, -5],
              test = test_set[, -5],
              cl = training_set[, 5],
              k=5,
              prob=TRUE)


cm <- table(test_set[,5],y_pred)
cm

>>>output

y_pred
             setosa versicolor virginica
  setosa         10          0         0
  versicolor      0          9         1
  virginica       0          1         9

plot(iris_data)




#############################
iris_data_2 <- iris_data[-c(2)]


set.seed(123)
split <- sampe.split(iris_data_2$Species, SplitRatio=0.8)
training_set <- subset(iris_data_2,split==TRUE)
test_set <- subset(iris_data_2,split==FALSE)



training_set[-4]<- scale(training_set[,-4])
test_set[-4]<-scale(test_set[-4])




y_pred <-knn(train = training_set[,-4],
             test = test_set[,-4],
             cl = training_set[,4],
             k=5,
             prob=TRUE)
cm2 <- table(test_set[,4],y_pred)
cm2



>>>>output 


y_pred
             setosa versicolor virginica
  setosa         10          0         0
  versicolor      0         10         0
  virginica       0          1         9





##############################3
iris_data_2$Sum <- apply(iris_data_2[,c('Sepal.Length','Petal.Length','Petal.width')],
                         1. function(x) sum(x))

set.seed(123)
split <- sample.split(iris_data_2$Species,SplitRatio=0.8)
training_set <- subset(iris_data_2,split==TRUE)
test_set <- subset(iris_data_2,split==FALSE)

#feature scalling
training_set[-4] <- scale(training_set[-4])
test_set[-4]<-scale(test_set[-4])

y_pred <-knn(train = training_set[,-4],
             test = training_set[,-4],
             c1 = training_set[,4],
             k=5,
             prob=TRUE)
cm2 <- table(test_set[,4],y_pred)
cm2


>>>>output


y_pred
             setosa versicolor virginica
  setosa         10          0         0
  versicolor      0         10         0
  virginica       0          1         9



####################
#apply kfold cross validation
#install.packages('caret')
library(caret)
folds <- createFolds(training_set$Species,k=5)
cv <- lapply(folds,function(x){
  training_fold <- training_set[-x,]
  test_fold <- training_set[x,]
  
  y_pred <- knn(train=training_fold[,-4],
                test=test_fold[,-4],
                cl=training_fold[,4],
                k=5,
                prob=TRUE)
  
  cm2 <- table(test_fold[,4],y_pred)
  accuracy= (cm2[1,1]+ cm2[2,2]+cm2[3,3])/ (cm2[1,1]+cm2[2,2]+cm2[3,3]
                                            +cm2[1,2]+cm2[1,3]+cm2[2,1]
                                            +cm2[2,3]+cm2[3,1]+cm2[3,2])
  return (accuracy)
  
})

cv
accuracy <- mean(as.numeric(cv))
accuracy

>>>output 
$Fold1
[1] 0.9166667

$Fold2
[1] 0.9583333

$Fold3
[1] 1

$Fold4
[1] 0.9166667

$Fold5
[1] 1


accuracy
[1] 0.9583333



















###$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$############
####CLUSTERING
#install.packages("stats")
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("ggfortify")

library(stats)
library(dplyr)
library(ggplot2)
library(ggfortify)

#View(iris)
#unsupervised learning-hence converting iris data to unlabeled
mydata=select(iris, c(1,2,3,4))



#WSS function
wssplot <- function(data,nc=15,seed=1234)
{
  wss<-(nrow(data)-1)*sum(apply(data,2,var))
  for(i in 2:nc)
  {
    set.seed(seed)
    wss[i] <- sum(kmeans(data,centers=i)$withinss)
  }
  plot(1:nc,wss,type="b",xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}


#wss plot to chose maximum number of clusters
wssplot(mydata)

#Spotting the kink in the curve in order to choose the optimum 


#k-means cluster
km=kmeans(mydata,2)


#evaluating Cluster analysis



#cluster plot
autoplot(km,mydata,frame=TRUE) # graph or plot has no overlap

#cluster centres

km$centers




>>>>output

km$centers
  Sepal.Length Sepal.Width Petal.Length Petal.Width
1     6.301031    2.886598     4.958763    1.695876
2     5.005660    3.369811     1.560377    0.290566























#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$#

###dimensionality reduction
library(stats)
library(dplyr)


#unsupervised
mydata=select(iris,c(1,2,3,4))
#check PCA eligibility
#if average corelation is above 0.3 or below -0.3  then variables are highly correlated and eligible for PCA
cor(mydata)
mean(cor(mydata))


>>>output

Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
> mean(cor(mydata))
[1] 0.4675531



#pca

PCA=princomp(mydata)

#evaluating pca
#two ways:
#1. check wether pcs capture the essence of original variable
#2. check wether the pcs are independent

#pc loadings

PCA$loadings

>>>output 

Loadings:
             Comp.1 Comp.2 Comp.3 Comp.4
Sepal.Length  0.361  0.657  0.582  0.315
Sepal.Width          0.730 -0.598 -0.320
Petal.Length  0.857 -0.173        -0.480
Petal.Width   0.358        -0.546  0.754

               Comp.1 Comp.2 Comp.3 Comp.4
SS loadings      1.00   1.00   1.00   1.00
Proportion Var   0.25   0.25   0.25   0.25
Cumulative Var   0.25   0.50   0.75   1.00




#principal components
Pc=PCA$scores
View(Pc)
cor(Pc)

>>>output 

Comp.1        Comp.2        Comp.3        Comp.4
Comp.1  1.000000e+00 -2.845751e-16 -3.269553e-16 -3.617947e-15
Comp.2 -2.845751e-16  1.000000e+00  3.891797e-15  3.898034e-15
Comp.3 -3.269553e-16  3.891797e-15  1.000000e+00 -1.116216e-14
Comp.4 -3.617947e-15  3.898034e-15 -1.116216e-14  1.000000e+00











